{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Scoring Exploration\n",
    "\n",
    "This notebook provides an interactive walkthrough of the **agent-mesh trust scoring system**.\n",
    "You'll learn how agents earn trust, how scores decay over time, and how policy violations\n",
    "affect an agent's ability to operate within the mesh.\n",
    "\n",
    "### The 5-Dimension Model\n",
    "\n",
    "| Dimension | Weight | What it measures |\n",
    "|-----------|--------|------------------|\n",
    "| Policy Compliance | 0.25 | Did the agent follow declared policies? |\n",
    "| Security Posture | 0.25 | Did the agent stay within trust boundaries? |\n",
    "| Output Quality | 0.20 | Did downstream consumers accept the output? |\n",
    "| Resource Efficiency | 0.15 | Was token/compute usage proportionate? |\n",
    "| Collaboration Health | 0.15 | Did inter-agent handoffs complete? |\n",
    "\n",
    "Scores range from **0 to 1000** and map to trust tiers:\n",
    "- **Verified Partner** (900+), **Trusted** (700-899), **Standard** (500-699), **Probationary** (300-499), **Untrusted** (<300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from agentmesh.reward.engine import RewardEngine, RewardConfig\n",
    "from agentmesh.reward.scoring import DimensionType, TrustScore, ScoreThresholds\n",
    "from agentmesh.reward.trust_decay import NetworkTrustEngine, TrustEvent\n",
    "from agentmesh.constants import (\n",
    "    TRUST_SCORE_DEFAULT, TRUST_REVOCATION_THRESHOLD,\n",
    "    TIER_VERIFIED_PARTNER_THRESHOLD, TIER_TRUSTED_THRESHOLD,\n",
    "    TIER_STANDARD_THRESHOLD, TIER_PROBATIONARY_THRESHOLD,\n",
    ")\n",
    "\n",
    "# Create three agents with different profiles\n",
    "AGENTS = {\n",
    "    \"did:mesh:alice\": \"Reliable data processor\",\n",
    "    \"did:mesh:bob\": \"New agent, still proving itself\",\n",
    "    \"did:mesh:carol\": \"Agent that will misbehave\",\n",
    "}\n",
    "\n",
    "engine = RewardEngine()\n",
    "\n",
    "print(\"Registered agents:\")\n",
    "for did, role in AGENTS.items():\n",
    "    score = engine.get_agent_score(did)\n",
    "    print(f\"  {did} — {role}\")\n",
    "    print(f\"    Score: {score.total_score}  Tier: {score.tier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Trust Scores Work\n",
    "\n",
    "Every agent starts at a **default score of 500** (Standard tier). The `RewardEngine`\n",
    "records signals across 5 dimensions and recalculates a weighted total score.\n",
    "\n",
    "Each signal has a value between **0.0** (bad) and **1.0** (good). Dimension scores\n",
    "use an exponential moving average, so recent signals matter more than old ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the default score breakdown for a fresh agent\n",
    "explanation = engine.get_score_explanation(\"did:mesh:alice\")\n",
    "\n",
    "print(f\"Agent: {explanation['agent_did']}\")\n",
    "print(f\"Total Score: {explanation['total_score']}\")\n",
    "print(f\"Trend: {explanation['trend']}\")\n",
    "print(f\"Revoked: {explanation['revoked']}\")\n",
    "print()\n",
    "\n",
    "print(\"Dimension Weights:\")\n",
    "for dim in DimensionType:\n",
    "    print(f\"  {dim.value:.<30} weight={getattr(engine.config, f'{dim.value}_weight', 0):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Score Thresholds:\")\n",
    "thresholds = ScoreThresholds()\n",
    "for tier, val in [\n",
    "    (\"Verified Partner\", thresholds.verified_partner),\n",
    "    (\"Trusted\", thresholds.trusted),\n",
    "    (\"Standard\", thresholds.standard),\n",
    "    (\"Probationary\", thresholds.probationary),\n",
    "    (\"Revocation\", thresholds.revocation_threshold),\n",
    "]:\n",
    "    print(f\"  {tier:.<25} {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Agent Interactions\n",
    "\n",
    "Let's simulate **Alice** performing well — completing tasks, staying in policy,\n",
    "and collaborating successfully. Watch her score climb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = \"did:mesh:alice\"\n",
    "score_history = []\n",
    "\n",
    "# Record 20 rounds of positive signals across all dimensions\n",
    "for i in range(20):\n",
    "    engine.record_policy_compliance(alice, compliant=True, policy_name=\"data-access\")\n",
    "    engine.record_security_event(alice, within_boundary=True, event_type=\"api_call\")\n",
    "    engine.record_output_quality(alice, accepted=True, consumer=\"did:mesh:bob\")\n",
    "    engine.record_resource_usage(alice, tokens_used=800, tokens_budget=1000,\n",
    "                                  compute_ms=150, compute_budget_ms=200)\n",
    "    engine.record_collaboration(alice, handoff_successful=True, peer_did=\"did:mesh:bob\")\n",
    "\n",
    "    # Recalculate and record\n",
    "    score = engine._recalculate_score(alice)\n",
    "    score_history.append(score.total_score)\n",
    "\n",
    "print(\"Alice's score over 20 interaction rounds:\")\n",
    "print()\n",
    "for i, s in enumerate(score_history):\n",
    "    bar = '█' * (s // 20)\n",
    "    print(f\"  Round {i+1:2d}: {s:4d} {bar}\")\n",
    "\n",
    "final = engine.get_agent_score(alice)\n",
    "print(f\"\\nFinal: score={final.total_score}, tier={final.tier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Decay\n",
    "\n",
    "The `NetworkTrustEngine` applies **linear temporal decay** — if an agent has no\n",
    "positive signals, its score decreases at a configurable rate (default: 2 points/hour).\n",
    "\n",
    "This prevents stale agents from retaining high trust indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "decay_engine = NetworkTrustEngine(decay_rate=2.0)\n",
    "\n",
    "# Start an agent at score 800\n",
    "agent = \"did:mesh:decay-demo\"\n",
    "decay_engine.set_score(agent, 800)\n",
    "\n",
    "# Simulate 48 hours of inactivity (no positive signals)\n",
    "base_time = time.time()\n",
    "decay_engine._last_positive[agent] = base_time\n",
    "\n",
    "decay_log = [(0, 800.0)]\n",
    "for hour in range(1, 49):\n",
    "    sim_time = base_time + hour * 3600\n",
    "    decay_engine.apply_temporal_decay(now=sim_time)\n",
    "    current = decay_engine.get_score(agent)\n",
    "    decay_log.append((hour, current))\n",
    "\n",
    "print(\"Trust decay over 48 hours of inactivity (rate=2.0/hr):\")\n",
    "print()\n",
    "for hour, score in decay_log[::4]:  # Every 4 hours\n",
    "    bar = '█' * int(score // 20)\n",
    "    print(f\"  Hour {hour:3d}: {score:6.1f} {bar}\")\n",
    "\n",
    "print(f\"\\nScore dropped from 800.0 → {decay_log[-1][1]:.1f} over 48 hours\")\n",
    "print(f\"Note: decay floors at 100 to allow recovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Violations & Trust Impact\n",
    "\n",
    "When an agent violates policies or causes security events, trust drops sharply.\n",
    "Critical signals (value < 0.3) trigger **immediate recalculation**.\n",
    "\n",
    "If the score falls below **300**, automatic credential revocation is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carol = \"did:mesh:carol\"\n",
    "revocations = []\n",
    "\n",
    "# Track revocations\n",
    "engine.on_revocation(lambda did, reason: revocations.append((did, reason)))\n",
    "\n",
    "# Carol starts with some good behavior\n",
    "for _ in range(5):\n",
    "    engine.record_policy_compliance(carol, compliant=True)\n",
    "    engine.record_security_event(carol, within_boundary=True, event_type=\"normal\")\n",
    "engine._recalculate_score(carol)\n",
    "print(f\"Carol after good behavior: {engine.get_agent_score(carol).total_score}\")\n",
    "\n",
    "# Now Carol starts violating policies\n",
    "violation_scores = []\n",
    "violations = [\n",
    "    (\"Policy violation: unauthorized data access\", DimensionType.POLICY_COMPLIANCE, 0.0),\n",
    "    (\"Security breach: left trust boundary\", DimensionType.SECURITY_POSTURE, 0.0),\n",
    "    (\"Output rejected by consumer\", DimensionType.OUTPUT_QUALITY, 0.1),\n",
    "    (\"Resource abuse: 10x over budget\", DimensionType.RESOURCE_EFFICIENCY, 0.0),\n",
    "    (\"Failed handoff to peer\", DimensionType.COLLABORATION_HEALTH, 0.0),\n",
    "    (\"Critical: attempted privilege escalation\", DimensionType.SECURITY_POSTURE, 0.0),\n",
    "]\n",
    "\n",
    "for desc, dim, val in violations:\n",
    "    engine.record_signal(carol, dim, val, source=\"monitor\", details=desc)\n",
    "    score = engine._recalculate_score(carol)\n",
    "    violation_scores.append((desc, score.total_score, score.tier))\n",
    "    print(f\"  ✗ {desc}\")\n",
    "    print(f\"    → Score: {score.total_score}, Tier: {score.tier}\")\n",
    "\n",
    "print()\n",
    "if revocations:\n",
    "    print(f\"⚠ REVOCATION triggered: {revocations[-1][1]}\")\n",
    "else:\n",
    "    print(\"No revocation triggered (score stayed above threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Threshold Gates\n",
    "\n",
    "The `ScoreThresholds` class provides gate decisions — **allow**, **warn**, or **revoke**\n",
    "based on the current score. This is how the mesh decides whether an agent can perform actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = ScoreThresholds()\n",
    "\n",
    "test_scores = [950, 750, 500, 400, 300, 200, 100]\n",
    "\n",
    "print(f\"{'Score':>6}  {'Tier':<20} {'Allow?':<8} {'Warn?':<8} {'Revoke?':<8}\")\n",
    "print(f\"{'─'*6}  {'─'*20} {'─'*8} {'─'*8} {'─'*8}\")\n",
    "\n",
    "for score in test_scores:\n",
    "    tier = thresholds.get_tier(score)\n",
    "    allow = thresholds.should_allow(score)\n",
    "    warn = thresholds.should_warn(score)\n",
    "    revoke = thresholds.should_revoke(score)\n",
    "    print(f\"{score:>6}  {tier:<20} {'✓' if allow else '✗':<8} {'⚠' if warn else '—':<8} {'☠' if revoke else '—':<8}\")\n",
    "\n",
    "print()\n",
    "print(\"Gate logic:\")\n",
    "print(f\"  Allow  → score >= {thresholds.allow_threshold}\")\n",
    "print(f\"  Warn   → score <  {thresholds.warn_threshold}\")\n",
    "print(f\"  Revoke → score <  {thresholds.revocation_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize how trust scores evolve for agents with different behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    HAS_MATPLOTLIB = False\n",
    "\n",
    "# Build score timelines for all three agents\n",
    "viz_engine = RewardEngine()\n",
    "\n",
    "timelines = {\"Alice (reliable)\": [], \"Bob (mixed)\": [], \"Carol (violator)\": []}\n",
    "agents_map = {\n",
    "    \"Alice (reliable)\": \"did:mesh:alice-viz\",\n",
    "    \"Bob (mixed)\": \"did:mesh:bob-viz\",\n",
    "    \"Carol (violator)\": \"did:mesh:carol-viz\",\n",
    "}\n",
    "\n",
    "for step in range(30):\n",
    "    # Alice: consistently good\n",
    "    viz_engine.record_policy_compliance(agents_map[\"Alice (reliable)\"], compliant=True)\n",
    "    viz_engine.record_security_event(agents_map[\"Alice (reliable)\"], True, \"normal\")\n",
    "    viz_engine.record_output_quality(agents_map[\"Alice (reliable)\"], True, \"peer\")\n",
    "\n",
    "    # Bob: mostly good, occasional issues\n",
    "    good = step % 4 != 0  # Fails every 4th round\n",
    "    viz_engine.record_policy_compliance(agents_map[\"Bob (mixed)\"], compliant=good)\n",
    "    viz_engine.record_security_event(agents_map[\"Bob (mixed)\"], good, \"api_call\")\n",
    "    viz_engine.record_output_quality(agents_map[\"Bob (mixed)\"], good, \"peer\")\n",
    "\n",
    "    # Carol: starts ok, then increasingly bad\n",
    "    carol_good = step < 10\n",
    "    viz_engine.record_policy_compliance(agents_map[\"Carol (violator)\"], compliant=carol_good)\n",
    "    viz_engine.record_security_event(agents_map[\"Carol (violator)\"], carol_good, \"check\")\n",
    "    if step >= 15:\n",
    "        viz_engine.record_signal(agents_map[\"Carol (violator)\"],\n",
    "                                  DimensionType.SECURITY_POSTURE, 0.0, \"monitor\")\n",
    "\n",
    "    for label, did in agents_map.items():\n",
    "        s = viz_engine._recalculate_score(did)\n",
    "        timelines[label].append(s.total_score)\n",
    "\n",
    "if HAS_MATPLOTLIB:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for label, scores in timelines.items():\n",
    "        ax.plot(range(1, 31), scores, marker='o', markersize=3, label=label)\n",
    "\n",
    "    # Threshold lines\n",
    "    ax.axhline(y=TIER_TRUSTED_THRESHOLD, color='green', linestyle='--', alpha=0.5, label='Trusted (700)')\n",
    "    ax.axhline(y=TIER_STANDARD_THRESHOLD, color='orange', linestyle='--', alpha=0.5, label='Standard (500)')\n",
    "    ax.axhline(y=TRUST_REVOCATION_THRESHOLD, color='red', linestyle='--', alpha=0.5, label='Revocation (300)')\n",
    "\n",
    "    ax.set_xlabel('Interaction Round')\n",
    "    ax.set_ylabel('Trust Score (0-1000)')\n",
    "    ax.set_title('Trust Score Evolution by Agent Behavior')\n",
    "    ax.legend(loc='lower left', fontsize=8)\n",
    "    ax.set_ylim(0, 1050)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Text-based fallback\n",
    "    print(\"Trust Score Evolution (matplotlib not installed — text fallback)\")\n",
    "    print()\n",
    "    for label, scores in timelines.items():\n",
    "        print(f\"{label}:\")\n",
    "        for i in range(0, 30, 5):\n",
    "            bar = '█' * (scores[i] // 25)\n",
    "            print(f\"  Round {i+1:2d}: {scores[i]:4d} {bar}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
